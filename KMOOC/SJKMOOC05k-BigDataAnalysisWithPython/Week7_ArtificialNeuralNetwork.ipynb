{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Netowrk\n",
    "\n",
    "![week7_1.png](images/week7_1.png)\n",
    "- 가지돌기(Dendrites) : 다른 뉴런으로부터의 입력을 받는 역할\n",
    "- 신경세포체(Cell Body) : 가지돌기로부터 입력 받은 값을 하나의 신호로 통합 후 신호 값이 어떤 임계값을 넘으면 하나의 신호 생성 \n",
    "- 축색(Axon) : 앞에서 만들어진 신호를 다른 신경세포로 전달\n",
    "\n",
    "![week7_2.png](images/week7_2.png)\n",
    "\n",
    "#### 활성화 함수\n",
    "- 출력값을 내보낼 때 사용하는 함수\n",
    "- 선형함수, 계단함수, 임계논리함수, 시그모이드함수 등이 사용됨\n",
    "- 데이터와 활성화 함수에 따라 출력값이 달라짐\n",
    "\n",
    "#### 단일 계층 신경망(Single Neural Network)\n",
    "![week7_3.png](images/week7_3.png)\n",
    "- Hidden Layer가 없이 Input Layer와 Output Layer로만 구성된 신경망\n",
    "- Input Layer의 노드와 Weight값들의 연산을 통해 활성화 함수로 전달 \n",
    "- 전달받은 값들 을활성화 함수를 이용해 계산 후 값을 출력하는 구조\n",
    "\n",
    "![week7_4.png](images/week7_4.png)\n",
    "![week7_5.png](images/week7_5.png)\n",
    "![week7_6.png](images/week7_6.png)\n",
    "\n",
    "#### 다층 신경망(MultiLayer Neural Network)\n",
    "![week7_7.png](images/week7_7.png)\n",
    "- 다층신경망은 입력, 은닉, 출력층으로 구분\n",
    "- 다층신경망은 하나 혹은 그 이상의 은닉층이 있는 신경망\n",
    "    - 입력층(Input Layer) : 입력신호(데이터)를 받아들여 은닉층의 모든 뉴런으로 재분배\n",
    "    - 은닉층(Hidden Layer) : 데이터의 특성을 파악. 은닉층 하나를 추가할 때마다 계산량은 지수적으로 늘어남\n",
    "    - 출력층(Output Layer) : 은닉층의 신호를 전달 받아 전체 신경망의 출력패턴을 정하는 역할 수행\n",
    "\n",
    "#### 역전파 알고리즘(Backpropagation Neural Network)\n",
    "- 오차값들을 조절해 가중치의 오차가 최소가 되게 하는 방법\n",
    "- 역방향으로 가중치 오차를 전파해 최적의 학습 결과를 찾아가는 것\n",
    "- 가중치의 오차가 최소가 되거나 정해진 오차 범위 내에 들어가는 경우 알고리즘은 종료\n",
    "\n",
    "![week7_8.png](images/week7_8.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris #data 불러오기\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.keys() #iris의 key값 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.feature_names\n",
    "# 'sepal length (cm)' : 꽃받침 길이\n",
    "# 'sepal width (cm)': 꽃받침 너비\n",
    "# 'petal length (cm)' : 꽃잎 길이\n",
    "# 'petal width (cm)' : 꽃잎 너비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris[\"data\"].shape #iris의 데이터에 해당하는 부분의 X와 Y의 크기를 나타냄\n",
    "#150행(자료 수), 4열(컬럼)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "       [ 4.9,  3. ,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.3,  0.2],\n",
       "       [ 4.6,  3.1,  1.5,  0.2],\n",
       "       [ 5. ,  3.6,  1.4,  0.2],\n",
       "       [ 5.4,  3.9,  1.7,  0.4],\n",
       "       [ 4.6,  3.4,  1.4,  0.3],\n",
       "       [ 5. ,  3.4,  1.5,  0.2],\n",
       "       [ 4.4,  2.9,  1.4,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " iris[\"data\"][:10] #iris데이터셋의 0번째부터 9번째까지를 슬라이싱해서 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], \n",
       "      dtype='<U10')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target_names #분류 목록 (정답 종류)\n",
    "#iris[\"target_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris[\"target\"] #target 변수의 값 출력\n",
    "#0~49는 0, 그 뒤 50개는 1, 그 뒤 50개는 2\n",
    "#0 = setosa, 1 = versicolor, 2 = virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = iris[\"data\"] #X에는 iris데이터의 값 150x4의 크기를 입력\n",
    "y = iris[\"target\"] #y에는 분류하고자 하는 target변수를 입력\n",
    "#target변수는 데이터가 무엇인지에 대해 판별하는 값\n",
    "#iris target의 경우 0, 1, 2로 구분됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #sklearn의 model_selection 내에 train_test_split를 로드\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) #데이터를 train과 test로 구분 #비율을 따로 정해주지 않는 경우 3:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 정규화\n",
    "from sklearn.preprocessing import StandardScaler #StandardScaler는 정규화를 시키는 함수\n",
    "\n",
    "scaler = StandardScaler() #StandardScaler는 데이터의 범위를 평균 0, 표준편차 1의 범위로 바꿔주는 함수 - 정규분포화. Nomalization\n",
    "scaler.fit(X_train) #데이터의 평균과 표준편차를 구한다. - 계수 추정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train) #정규화\n",
    "X_test = scaler.transform(X_test) #정규화\n",
    "\n",
    "#fit_transform() 메서드를 사용하여 계수 추정과 자료 변환을 동시에 실행할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n다층 신경망은 기본적인 활성화함수로 Relu 사용\\nRelu 활성화 함수는 음수일 경우 0, 양수일 경우 선형적 데이터를 출력\\nactivation : 다층신경망에서 사용하는 활성화 함수를 나타냄\\nalpha : 신경망 내의 정규화 파라미터\\nbatch_size : 최적화를 시키기 위한 학습 최소 크기\\nepsilon : 수치 안정성을 위한 오차 값\\nlearning_rate : 사용할 학습률의 형태. 기본값은 constant\\nlearning_rate_init : 학습률. 기본값은 0.001\\nmax_iter : 최대 반복 횟수\\nmomentum : gradient descent update를 위한 모멘텀. 기본 값은 0.9\\nhidden_layer_size : 은닉층 별 노드 수\\nnesterovs_momentum : nesterovs_momentum 사용 여부. solver = 'sgd' 및 momentum > 0인 경우에만 사용\\npower_t : 기본갑은 0.5 역 스케일링 학습률의 지수. learning_rate가 'invscaling'으로 설정된 경우 효과적인 학습 속도를 업데이트하는 데 사용\\n          solver = 'sgd'일 때만 사용된다.\\nrandom_state : int 또는 RandomSate. 기본값은 없음. 난수 생성기의 상태 또는 시드\\nshuffle : 반복할 때마다 데이터들의 위치를 임의적으로 변경하는지의 여부\\nsolver : 가중치 최적화를 위한 사용하는 함수(기본값으로 adam이라는 함수 사용)\\nvaridation_fraction : 모델을 만들 때 내부적으로 검증하기 위해 사용하는 데이터의 비율\\nvaridation : 데이터를 학습 시 데이터가 유의미한지를 검증하는 데이터\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier #다중인공신경망(MLP) 분류 알고리즘을 sklearn의 neural_network에서 로드\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,10,10)) #MLP 알고리즘의 히든레이어를 3계층(10,10,10)으로 할당\n",
    "mlp.fit(X_train, y_train) #학습\n",
    "\n",
    "\"\"\"\n",
    "다층 신경망은 기본적인 활성화함수로 Relu 사용\n",
    "Relu 활성화 함수는 음수일 경우 0, 양수일 경우 선형적 데이터를 출력\n",
    "activation : 다층신경망에서 사용하는 활성화 함수를 나타냄\n",
    "alpha : 신경망 내의 정규화 파라미터\n",
    "batch_size : 최적화를 시키기 위한 학습 최소 크기\n",
    "epsilon : 수치 안정성을 위한 오차 값\n",
    "learning_rate : 사용할 학습률의 형태. 기본값은 constant\n",
    "learning_rate_init : 학습률. 기본값은 0.001\n",
    "max_iter : 최대 반복 횟수\n",
    "momentum : gradient descent update를 위한 모멘텀. 기본 값은 0.9\n",
    "hidden_layer_size : 은닉층 별 노드 수\n",
    "nesterovs_momentum : nesterovs_momentum 사용 여부. solver = 'sgd' 및 momentum > 0인 경우에만 사용\n",
    "power_t : 기본갑은 0.5 역 스케일링 학습률의 지수. learning_rate가 'invscaling'으로 설정된 경우 효과적인 학습 속도를 업데이트하는 데 사용\n",
    "          solver = 'sgd'일 때만 사용된다.\n",
    "random_state : int 또는 RandomSate. 기본값은 없음. 난수 생성기의 상태 또는 시드\n",
    "shuffle : 반복할 때마다 데이터들의 위치를 임의적으로 변경하는지의 여부\n",
    "solver : 가중치 최적화를 위한 사용하는 함수(기본값으로 adam이라는 함수 사용)\n",
    "varidation_fraction : 모델을 만들 때 내부적으로 검증하기 위해 사용하는 데이터의 비율\n",
    "varidation : 데이터를 학습 시 데이터가 유의미한지를 검증하는 데이터\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test) #mlp로 학습한 내용을 X_test에 대해 예측하여 predictions변수에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  0  0]\n",
      " [ 0 19  0]\n",
      " [ 0  0 13]]\n"
     ]
    }
   ],
   "source": [
    "#sklearn.metrics의 confusion_matrix와 classification_report를 로드\n",
    "#confusion_matrix는 데이터가 맞는지의 유무를 판단\n",
    "#classification_report는 precision과 recall 그리고 f1_score등을 계산해 정확률에 대해 계산\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#confusion_matrix를 이용해 실제값 y_test와 예측값에 대해 비교\n",
    "print(confusion_matrix(y_test, predictions)) #실제 값과 예측값 비교\n",
    "\n",
    "#[[ 0을 0로 분류, 0을 1로 분류, 0을 2로 분류]\n",
    "# [ 1을 0로 분류, 1을 1로 분류, 1을 2로 분류]\n",
    "# [ 2을 0로 분류, 2을 1로 분류, 2을 2로 분류]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         6\n",
      "          1       1.00      1.00      1.00        19\n",
      "          2       1.00      1.00      1.00        13\n",
      "\n",
      "avg / total       1.00      1.00      1.00        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification_report를 이용해 정확률, 재현율, f1-score를 확인  \n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 과제\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.933333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits() # digits 데이터\n",
    "\n",
    "X = digits.data #feature 값\n",
    "y = digits.target #target 값\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2) #분류\n",
    "\n",
    "scaler = StandardScaler() #데이터 정규화 함수 생성\n",
    "scaler.fit_transform(X_train) #데이터 정규화\n",
    "scaler.fit_transform(X_test) #데이터 정규화\n",
    "\n",
    "ANN = MLPClassifier(hidden_layer_sizes=(10, 10, 10)).fit(X_train, y_train) #모델 생성, 학습\n",
    "print(ANN.score(X=X_test, y=y_test)) #accuracy 값 측정"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
